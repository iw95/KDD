\ifx\pdfminorversion\undefined\else\pdfminorversion=4\fi
\documentclass[aspectratio=169,t,table]{beamer}
%\documentclass[aspectratio=169,t,handout]{beamer}

% English version FAU Logo
\usepackage[english]{babel}
% German version FAU Logo
%\usepackage[ngerman]{babel}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{url}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{fontawesome}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{calc}
\usepackage{ifthen}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{tikz-cd}
\usepackage{verbatim}
\usepackage{pgfplots,pgfplotstable,pgf-pie}
\usepackage{filecontents}
\newcommand{\plots}{0.611201}
\newcommand{\plotm}{2.19882}
\pgfplotsset{height=4cm,width=8cm,compat=1.16}
\pgfmathdeclarefunction{gauss}{2}{%
  \pgfmathparse{1/(#2*sqrt(2*pi))*exp(-((x-#1)^2)/(2*#2^2))}%
}

\tikzset{
    vertex/.style = {
        circle,
        fill            = black,
        outer sep = 2pt,
        inner sep = 1pt,
    }
}

\tikzset{
    mynode/.style={
        draw,
        thick,
        anchor=south west,
        minimum width=2cm,
        minimum height=1.3cm,
        align=center,
        inner sep=0.2cm,
        outer sep=0,
        rectangle split,
        rectangle split parts=2,
        rectangle split draw splits=false},
    reverseclip/.style={
        insert path={(current page.north east) --
            (current page.south east) --
            (current page.south west) --
            (current page.north west) --
            (current page.north east)}
    }
}

\tikzset{basic/.style={
        draw,
        rectangle split,
        rectangle split parts=2,
        rectangle split part fill={blue!20,white},
        minimum width=2.5cm,
        text width=2cm,
        align=left,
        font=\itshape
    },
    Diamond/.style={ diamond,
                      draw,
                      shape aspect=2,
                      inner sep = 2pt,
                      text centered,
                      fill=blue!10!white,
                      font=\itshape
                    }}


\tikzset{level 1/.append style={sibling angle=50,level distance = 165mm}}
\tikzset{level 2/.append style={sibling angle=20,level distance = 45mm}}
\tikzset{every node/.append style={scale=1}}

\usetikzlibrary{arrows,decorations.pathmorphing,backgrounds,fit,positioning,shapes.symbols,chains,intersections,snakes,positioning,matrix,mindmap,shapes.multipart,shapes,calc,shapes.geometric}

% read in data file


\newcommand{\MaxNumberX}{3}
\newcommand{\MaxNumberY}{5}
\newcommand{\tikzmark}[1]{\tikz[remember picture] \node[coordinate] (#1) {#1};}

\pgfplotstableread{data/iris.dat}\iris
\pgfplotstablegetrowsof{\iris}
\pgfplotsset{compat=1.14}
\pgfmathsetmacro\NumRows{\pgfplotsretval-1}
\definecolor{airforceblue}{rgb}{0.36, 0.54, 0.66}

\usepgfplotslibrary{groupplots}
% Options:
%  - inst:      Institute
%                 med:      MedFak FAU theme
%                 nat:      NatFak FAU theme
%                 phil:     PhilFak FAU theme
%                 rw:       RWFak FAU theme
%                 rw-jura:  RWFak FB Jura FAU theme
%                 rw-wiso:  RWFak FB WISO FAU theme
%                 tf:       TechFak FAU theme
%  - image:     Cover image on title page
%  - plain:     Plain title page
%  - longtitle: Title page layout for long title
\usetheme[%
  image,%
  longtitle,%
  tf
]{fau}

% Enable semi-transparent animation preview
\setbeamercovered{transparent}


\lstset{%
  language=Python,
  tabsize=2,
  basicstyle=\tt,
  keywordstyle=\color{blue},
  commentstyle=\color{green!50!black},
  stringstyle=\color{red},
  numbers=left,
  numbersep=0.5em,
  xleftmargin=1em,
  numberstyle=\tt
}


% Title, authors, and date
\title[KDD]{Chapter VI: Classification}
\subtitle{Knowledge Discovery in Databases}
\author[L.~Melodia]{Luciano Melodia M.A.}
% English version
\institute[Department]{Evolutionary Data Management, Friedrich-Alexander University Erlangen-NÃ¼rnberg}
% German version
%\institute[Lehrstuhl]{Lehrstuhl, Friedrich-Alexander-Universit\"at Erlangen-N\"urnberg}
\date{Summer semester 2021}
% Set additional logo (overwrites FAU seal)
%\logo{\includegraphics[width=.15\textwidth]{themefau/art/xxx/xxx.pdf}}
\begin{document}
  % Title
  \maketitle

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Chapter VI: Classification}
        \begin{itemize}
            \item \textbf{Classification: basic concepts.}
            \item Decision-tree induction.
            \item Bayes classification methods.
            \item Rule-based classification.
            \item Model evaluation and selection.
            \item Techniques to improve classification accuracy: ensemble methods.
            \item Summary.
        \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Supervised vs. unsupervised learning}
        \begin{itemize}
            \item \textbf{\color{airforceblue}Supervised learning (classification).}
            \begin{itemize}
              \item Supervision:
              \begin{itemize}
                \item The \textbf{training data} (observations, measurements, etc.) are accompanied by \textbf{labels} indicating the \textbf{class} of the observations.
                \item New data is classified based on a \textbf{model} created from the training data.
              \end{itemize}
            \end{itemize}
            \item \textbf{\color{airforceblue}Unsupervised learning (clustering).}
            \begin{itemize}
              \item The class labels of training data are unknown.
              \begin{itemize}
                \item Or rather, there are no training data.
              \end{itemize}
            \end{itemize}
            \begin{itemize}
              \item Given a set of measurements, observations, etc., \\ the goal is to find classes or clusters in the data.
              \begin{itemize}
                \item See next chapter.
              \end{itemize}
            \end{itemize}
        \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Prediction problems: classification vs. numerical prediction}
        \begin{itemize}
            \item \textbf{Classification:}
            \begin{itemize}
              \item Predicts \textbf{\color{airforceblue}categorical class labels} (discrete, nominal).
              \item Constructs a model based on the training set and the values (class labels) in a classifying attribute and uses it in classifying new data.
            \end{itemize}
            \item \textbf{Numerical prediction:}
            \begin{itemize}
              \item Models \textbf{\color{airforceblue}continuous-valued functions}.
              \item I.e. predicts missing or unknown (future) values.
            \end{itemize}
            \item \textbf{Typical applications of classification:}
            \begin{itemize}
              \item Credit/loan approval: Will it be paid back?
              \item Medical diagnosis: Is a tumor cancerous or benign?
              \item Fraud detection: Is a transaction fraudulent or not?
              \item Web-page categorization: Which category is it?
            \end{itemize}
        \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Classification -- a two-step process}
        \begin{itemize}
            \item \textbf{Model construction: describing a set of predetermined classes:}
            \begin{itemize}
              \item Each tuple/sample is assumed to belong to a predefined class, as determined by the \textbf{\color{airforceblue}class-label attribute}.
              \item The set of tuples used for model construction is the \textbf{\color{airforceblue}training set}.
              \item The \textbf{\color{airforceblue}model} is represented as classification rules, decision trees, or mathematical formulae.
            \end{itemize}
            \item \textbf{Model usage, for classifying future or unknown objects:}
            \begin{itemize}
              \item Estimate \textbf{\color{airforceblue}accuracy} of the model:
              \begin{itemize}
                \item The known label of \textbf{test samples} is compared with the result from the model.
                \item \textbf{Accuracy rate} is the percentage of test-set samples that are correctly classified by the model.
                \item Test set is independent of training set (otherwise overfitting).
              \end{itemize}
              \item If the accuracy is acceptable, \textbf{\color{airforceblue}use the model} to classify data tuples whose class labels are not known.
            \end{itemize}
        \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Classification -- a two-step process}
      \centering
      \begin{tikzpicture}
        \node at (0,0) {
        \begin{tabular}{|l|l|c|c|}
          \hline
          \cellcolor{blue!25}\textbf{\uppercase{name}} & \cellcolor{blue!25}\textbf{\uppercase{rank}} & \cellcolor{blue!25}\textbf{\uppercase{years}} & \cellcolor{brown!25}\textbf{\uppercase{tenured}} \\\hline
          \cellcolor{yellow!25}Mike & \cellcolor{yellow!25}Assistant Prof & \cellcolor{yellow!25}3 & \cellcolor{red!15}no \\\hline
          \cellcolor{yellow!25}Mary & \cellcolor{yellow!25}Assistant Prof & \cellcolor{yellow!25}7 & \cellcolor{green!15}yes \\\hline
          \cellcolor{yellow!25}Bill & \cellcolor{yellow!25}Professor & \cellcolor{yellow!25}2 & \cellcolor{green!15}yes \\\hline
          \cellcolor{yellow!25}Jim & \cellcolor{yellow!25}Associate Prof & \cellcolor{yellow!25}7 & \cellcolor{green!15}yes \\\hline
          \cellcolor{yellow!25}Dave & \cellcolor{yellow!25}Assistant Prof & \cellcolor{yellow!25}6 & \cellcolor{red!15}no \\\hline
          \cellcolor{yellow!25}Anne & \cellcolor{yellow!25}Associate Prof & \cellcolor{yellow!25}3 & \cellcolor{red!15}no \\\hline
        \end{tabular}
        };
        \node[fill=orange!50, text width = 3cm, align=center] at (7.5,5) {Classification};
        \node[fill=orange!50, text width = 3cm, align=center] at (7.5,4.6) {algorithms};

        \node[fill=green!20, text width = 3cm, align=left] at (7.5,0) {\texttt{if RANK =}'Professor'};
        \node[fill=green!20, text width = 3cm, align=left] at (7.5,-0.4) {\texttt{or YEARS >}6};
        \node[fill=green!20, text width = 3cm, align=left] at (7.5,-0.8) {\texttt{then TENURED =}'yes'};

        \node[text width = 3cm, align=center] at (0.05,3.7) {Training};
        \node[text width = 3cm, align=center] at (0.05,3.3) {data};

        \node[text width = 3cm, align=center] at (7.6,2.2) {Classifier};
        \node[text width = 3cm, align=center] at (7.6,1.8) {(model)};

        \draw [thick](-1.5,3.15) -- (-1.5,4.65);
        \draw [thick](1.5,3.15) -- (1.5,4.65);
        \draw [thick](-1.5,3.15) arc (180:360:1.5 and 0.5);
        \draw [thick](-1.5,4.65) arc (180:360:1.5 and 0.5);
        \draw [thick](1.5,4.65) arc (-1.5:180:1.5 and 0.5);
        \draw [thick, ->] (2,4.7) -- (5.5,4.7);
        \draw [thick, ->] (7.5,4.2) -- (7.5,3.8);
        \draw [thick] (6.05,1.5) -- (5.9,0.2);
        \draw [thick] (8.95,1.5) -- (9.1,0.2);

        \draw [thick](6,1.65) -- (6,3.15);
        \draw [thick](9,1.65) -- (9,3.15);
        \draw [thick](6,1.65) arc (180:360:1.5 and 0.5);
        \draw [thick](6,3.15) arc (180:360:1.5 and 0.5);
        \draw [thick](9,3.15) arc (-1.5:180:1.5 and 0.5);
        \draw [thick] (-3.7,1.5) -- (-1.5,3);
        \draw [thick] (1.5,3) -- (3.7,1.5);
      \end{tikzpicture}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Process (II): using the model in prediction}
      \centering
      \begin{tikzpicture}
        \draw [thick](-1.5,3.15) -- (-1.5,4.65);
        \draw [thick](1.5,3.15) -- (1.5,4.65);
        \draw [thick](-1.5,3.15) arc (180:360:1.5 and 0.5);
        \draw [thick](-1.5,4.65) arc (180:360:1.5 and 0.5);
        \draw [thick](1.5,4.65) arc (-1.5:180:1.5 and 0.5);
        \draw [thick, ->] (2,4.7) -- (5.5,4.7);
        \draw [thick, ->] (7.5,4.2) -- (7.5,3.8);
        \draw [thick] (6.05,1.5) -- (5.9,0.2);
        \draw [thick] (8.95,1.5) -- (9.1,0.2);
        \node[text width = 3cm, align=center] at (0.05,3.7) {Training};
        \node[text width = 3cm, align=center] at (0.05,3.3) {data};

        \node[text width = 3cm, align=center] at (7.6,2.2) {Unseen};
        \node[text width = 3cm, align=center] at (7.6,1.8) {data};

        \node at (0,0.4){
        \begin{tabular}{|l|l|c|c|}
          \hline
          \cellcolor{blue!20}\textbf{\uppercase{name}} & \cellcolor{blue!20}\textbf{\uppercase{rank}} & \cellcolor{blue!20}\textbf{\uppercase{years}} & \cellcolor{brown!20}\textbf{\uppercase{tenured}} \\\hline
          Tom & Assistant Prof & 2 & no \\\hline
          \cellcolor{green!20}Merlisa & \cellcolor{green!20}Associate Prof & \cellcolor{green!20}7 & \cellcolor{green!20}no \\\hline
          George & Professor & 5 & yes \\\hline
          Joseph & Assistant Prof & 7 & yes \\\hline
        \end{tabular}
        };
        \node[fill=orange!50, text width = 3cm, align=center] at (7.5,5) {Classification};
        \node[fill=orange!50, text width = 3cm, align=center] at (7.5,4.6) {algorithms};
        \node[fill=green!20, text width = 3cm, align=center] at (7.5,0) {(Jeff, Professor,4)};
        \draw [thick] (6.05,1.5) -- (5.9,0.2);
        \draw [thick] (8.95,1.5) -- (9.1,0.2);
        \draw [thick](6,1.65) -- (6,3.15);
        \draw [thick](9,1.65) -- (9,3.15);
        \draw [thick](6,1.65) arc (180:360:1.5 and 0.5);
        \draw [thick](6,3.15) arc (180:360:1.5 and 0.5);
        \draw [thick](9,3.15) arc (-1.5:180:1.5 and 0.5);
        \draw [thick] (-3.7,1.5) -- (-1.5,3);
        \draw [thick] (1.5,3) -- (3.7,1.5);
      \end{tikzpicture}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Chapter VI: Classification}
        \begin{itemize}
            \item Classification: basic concepts.
            \item \textbf{Decision-tree induction.}
            \item Bayes classification methods.
            \item Rule-based classification.
            \item Model evaluation and selection.
            \item Techniques to improve classification accuracy: ensemble methods.
            \item Summary.
        \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Decision-tree induction: an example}
      \begin{columns}
        \begin{column}{0.4\textwidth}
          \vspace{-3cm}
          \begin{itemize}
            \item \textbf{Training dataset: buys\_computer.}
            \begin{itemize}
              \item The dataset follows an example of Quinlan's ID3 (playing tennis).
            \end{itemize}
            \item \textbf{Resulting tree:}\\[0.1cm]
            \centering
            \begin{tikzpicture}
              \node[draw, fill=blue!20] at (0,0) (a) {age?};
              \node[fill=yellow!20] at (-1.5,-0.7) (b) {<=30};
              \node[fill=yellow!20] at (0,-0.7) (c) {$31\ldots40$};
              \node[fill=yellow!20] at (1.5,-0.7) (d) {$>40$};
              \node[draw, fill=blue!20] at (-1.5,-1.4) (e) {student?};
              \node[fill=yellow!20] at (-2,-2.1) (eno1) {no};
              \node[fill=yellow!20] at (-1,-2.1) (eyes1) {yes};
              \node[fill=green!20] at (-2,-2.8) (eno2) {no};
              \node[fill=green!20] at (-1,-2.8) (eyes2) {yes};
              \node[draw, fill=blue!20] at (1.5,-1.4) (g) {credit rating?};
              \node[fill=yellow!20] at (2,-2.1) (gf) {fair};
              \node[fill=yellow!20] at (1,-2.1) (gex) {excellent};
              \node[fill=orange!20] at (1,-2.8) (gno) {no};
              \node[fill=green!20] at (2,-2.8) (gyes) {yes};
              \node[fill=green!20] at (0,-1.4) (f) {yes};

              \draw (a)--(b);
              \draw (a)--(c);
              \draw (a)--(d);
              \draw (b)--(e);
              \draw (c)--(f);
              \draw (d)--(g);
              \draw (e)--(eno1);
              \draw (e)--(eyes1);
              \draw (eno1)--(eno2);
              \draw (eyes1)--(eyes2);
              \draw (g)--(gex);
              \draw (g)--(gf);
              \draw (gex)--(gno);
              \draw (gf)--(gyes);
            \end{tikzpicture}
          \end{itemize}
        \end{column}
        \begin{column}{0.6\textwidth}
          \begin{tabular}{|l|l|c|c|c|}
            \hline
            \cellcolor{blue!20}age & \cellcolor{blue!20}income & \cellcolor{blue!20}student & \cellcolor{blue!20}credit\_rating & \cellcolor{blue!20}buys\_coputer \\\hline
            \cellcolor{yellow!20}$\leq 30$ & \cellcolor{yellow!20}high & \cellcolor{yellow!20}no & \cellcolor{yellow!20}fair & \cellcolor{red!20}no \\\hline
            \cellcolor{yellow!20}$\leq 30$ & \cellcolor{yellow!20}high & \cellcolor{yellow!20}no & \cellcolor{yellow!20}excellent & \cellcolor{red!20}no \\\hline
            \cellcolor{yellow!20}$31\ldots40$ & \cellcolor{yellow!20}high & \cellcolor{yellow!20}no & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$>40$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}no & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$>40$ & \cellcolor{yellow!20}low & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$>40$ & \cellcolor{yellow!20}low & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}excellent & \cellcolor{red!20}no \\\hline
            \cellcolor{yellow!20}$31\ldots40$ & \cellcolor{yellow!20}low & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}excellent & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$\leq 30$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}no & \cellcolor{yellow!20}fair & \cellcolor{red!20}no \\\hline
            \cellcolor{yellow!20}$\leq 30$ & \cellcolor{yellow!20}low & \cellcolor{yellow!20}no & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$>40$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$\leq 30$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}excellent & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$31\ldots40$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}no & \cellcolor{yellow!20}excellent & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$31\ldots40$ & \cellcolor{yellow!20}high & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$>40$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}no & \cellcolor{yellow!20}excellent & \cellcolor{red!20}no \\\hline
          \end{tabular}
        \end{column}
      \end{columns}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Algorithm for decision-tree induction}
        \begin{itemize}
            \item \textbf{Basic algorithm (a greedy algorithm):}
            \begin{itemize}
              \item Tree is constructed in a \textbf{\color{airforceblue}top-down recursive divide-and-conquer manner.}
              \item Attributes are categorical.
              \begin{itemize}
                \item If not: discretize in advance.
              \end{itemize}
              \item At start, all the training examples are at the root.
              \item Examples are \textbf{\color{airforceblue}partitioned recursively} based on selected attributes.
              \item Test attributes are selected on the basis of a heuristic or statistical measure.
              \begin{itemize}
                \item E.g. information gain -- see on the next slide.
              \end{itemize}
            \end{itemize}
            \item \textbf{Conditions for stopping partitioning:}
            \begin{itemize}
              \item All samples for a given node belong to the same class.
              \item There are no remaining attributes for further partitioning.
              \begin{itemize}
                \item Majority voting is employed for classifying the leaf.
              \end{itemize}
              \item There are no samples left (i.e. partition for particular value is empty).
            \end{itemize}
        \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Attribute-selection measure: information gain (ID3/C4.5)}
      \begin{itemize}
        \item \textbf{Select the attribute with the highest information gain.}
        \begin{itemize}
          \item Let $p_i$ be the probability that an arbitrary tuple in $D$ belings to class $C_i$,\\ estimated by $\frac{|C_i|}{|D|}$, such that $1 \leq i \leq m$.
          \item \textbf{Expected information} (entropy)needed to classify a tuple in $D$:
          \begin{align}
            \text{Info}(D) = -\sum_{i=1}^{m}p_i \log_2(p_i).
          \end{align}
          \item \textbf{Information} needed (after using attribute $A$ to split $D$ into $v$ partitions) to classify $D$:
          \begin{align}
            \text{Info}_A(D) = \sum_{j=1}^v \left( \frac{|D_j|}{|D|} \text{Info}(D_j) \right).
          \end{align}
          \item \textbf{Information gained} by branching on $A$:
          \begin{align}
            \text{Gain}(A)=\text{Info}(D)-\text{Info}_A(D).
          \end{align}
        \end{itemize}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Attribute selection: information gain}
      \begin{columns}
        \begin{column}{0.45\textwidth}
          \begin{itemize}
            \item \textbf{Class P: buys\_computer = "yes"}
            \item \textbf{Class N: buys\_computer = "no"}
            \begin{align*}
              \resizebox{7cm}{!}{%
                $\text{Info}(D) = I(9,5) = - \frac{9}{14}\log_2(\frac{9}{14})-\frac{5}{14} \log_2(\frac{5}{14}) = 0.94$
              }
            \end{align*}
          \end{itemize}
          \centering
          \begin{tabular}{|l|l|l|l|}
            \hline
            \cellcolor{blue!20}age & \cellcolor{blue!20}p & \cellcolor{blue!20}n & \cellcolor{blue!20}$l(p,n)$ \\\hline
            \cellcolor{yellow!20}$\leq 30$ & 2 & 3 & 0.971 \\\hline
            \cellcolor{yellow!20}$31\ldots40$ & 4 & 0 & 0 \\\hline
            \cellcolor{yellow!20}$>40$ & 3 & 2 & 0.971 \\\hline
          \end{tabular}\\[0.2cm]
        \begin{itemize}
          \item \textbf{Similarly,}
          \begin{itemize}
            \item $\text{Gain}(\texttt{income}) = 0.029$,
            \item $\text{Gain}(\texttt{student}) = 0.151$,
            \item $\text{Gain}(\texttt{credit\_rating}) = 0.048$.
          \end{itemize}
        \end{itemize}
        \end{column}
        \begin{column}{0.45\textwidth}
          \vspace{-1.3cm}
          \begin{align*}
            \resizebox{7cm}{!}{%
              $\text{Info}_{\texttt{age}}(D) = \frac{5}{14}I(2,3) + \frac{4}{14} I(4,0) + \frac{5}{14} I(3,2) = 0.694$.
            }
          \end{align*}
          $\frac{5}{14} I(2,3)$ means "$\texttt{age} \leq 30$" has $5$ out of $14$ samples, with $2$ yes'es and $3$ no's. Hence,
          \begin{align*}
              \text{Gain}(\texttt{age}) = \text{Info}(D)-\text{Info}_{\texttt{age}}(D) = 0.246.
          \end{align*}
          \resizebox{\columnwidth}{!}{%
          \begin{tabular}{|l|l|c|l|c|}
            \hline
            \cellcolor{blue!20}age & \cellcolor{blue!20}income & \cellcolor{blue!20}student & \cellcolor{blue!20}credit\_rating & \cellcolor{brown!20}buys\_computer \\\hline
            \cellcolor{yellow!20}$\leq30$ & \cellcolor{yellow!20}high & \cellcolor{yellow!20}no & \cellcolor{yellow!20}fair & \cellcolor{red!20}no \\\hline
            \cellcolor{yellow!20}$\leq30$ & \cellcolor{yellow!20}high & \cellcolor{yellow!20}no & \cellcolor{yellow!20}excellent & \cellcolor{red!20}no \\\hline
            \cellcolor{yellow!20}$31\ldots40$ & \cellcolor{yellow!20}high & \cellcolor{yellow!20}no & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$>40$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}no & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$>40$ & \cellcolor{yellow!20}low & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$>40$ & \cellcolor{yellow!20}low & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}excellent & \cellcolor{red!20}no \\\hline
            \cellcolor{yellow!20}$31\ldots40$ & \cellcolor{yellow!20}low & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}excellent & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$\leq30$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}no & \cellcolor{yellow!20}fair & \cellcolor{red!20}no \\\hline
            \cellcolor{yellow!20}$>40$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$\leq30$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}excellent & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$31\ldots40$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}no & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$31\ldots40$ & \cellcolor{yellow!20}high & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$>40$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}no & \cellcolor{yellow!20}excellent & \cellcolor{red!20}no \\\hline
          \end{tabular}}
        \end{column}
      \end{columns}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Partitioning in the example}
      \centering
      \begin{tikzpicture}
        \node[draw, fill=blue!20] at (0,0) (r) {age?};
        \node[draw, fill=yellow!20] at (-2,-1) (a) {$\leq 30$};
        \node[draw, fill=yellow!20] at (0,-1) (b) {$31\ldots40$};
        \node[draw, fill=yellow!20] at (2,-1) (c) {$>40$};
        \draw (r)--(a);
        \draw (r)--(b);
        \draw (r)--(c);
        \node at (-4,-3) (t1) {
        \begin{tabular}{|l|l|l|l|}
          \hline
          \cellcolor{blue!20}income & \cellcolor{blue!20}student & \cellcolor{blue!20}credit\_rating & \cellcolor{brown!20}buys\_computer \\\hline
          \cellcolor{yellow!20}high & \cellcolor{yellow!20}no & \cellcolor{yellow!20}fair & n\cellcolor{red!20}o \\\hline
          \cellcolor{yellow!20}high & \cellcolor{yellow!20}no & \cellcolor{yellow!20}excellent & \cellcolor{red!20}no \\\hline
          \cellcolor{yellow!20}medium & \cellcolor{yellow!20}no & \cellcolor{yellow!20}fair & \cellcolor{red!20}no \\\hline
          \cellcolor{yellow!20}low & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
          \cellcolor{yellow!20}medium & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}excellent & \cellcolor{green!20}yes \\\hline
        \end{tabular}
        };
        \node at (-2.5,-5.2) (t3) {
        \begin{tabular}{|l|l|l|l|}
          \hline
          \cellcolor{blue!20}income & \cellcolor{blue!20}student & \cellcolor{blue!20}credit\_rating & \cellcolor{brown!20}buys\_computer \\\hline
          \cellcolor{yellow!20}high & \cellcolor{yellow!20}no & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
          \cellcolor{yellow!20}low & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}excellent & \cellcolor{green!20}yes \\\hline
          \cellcolor{yellow!20}medium & \cellcolor{yellow!20}no & \cellcolor{yellow!20}excellent & \cellcolor{green!20}yes \\\hline
          \cellcolor{yellow!20}high & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
        \end{tabular}
        };
        \node at (2.8,-3.8) (t2) {
        \begin{tabular}{|l|l|l|l|}
          \hline
          \cellcolor{blue!20}income & \cellcolor{blue!20}student & \cellcolor{blue!20}credit\_rating & \cellcolor{brown!20}buys\_computer \\\hline
          \cellcolor{yellow!20}medium & \cellcolor{yellow!20}no & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
          \cellcolor{yellow!20}low & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
          \cellcolor{yellow!20}low & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}excellent & \cellcolor{red!20}no \\\hline
          \cellcolor{yellow!20}medium & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
          \cellcolor{yellow!20}medium & \cellcolor{yellow!20}no & \cellcolor{yellow!20}excellent & \cellcolor{red!20}no \\\hline
        \end{tabular}
        };
        \draw (a)--(t1);
        \draw (c)--(t2);
        \draw (b)--(t3);
      \end{tikzpicture}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Computing information gain for continuous-valued attributes}
      \begin{itemize}
        \item \textbf{Let attribute A be a continuous-valued attribute.}
        \item \textbf{Must determine the best split point for A.}
        \begin{itemize}
          \item Sort the values of A in increasing order.
          \item Typically, the midpoint between each pair of adjacent values \\ is considered as a possible split point.
          \begin{itemize}
            \item $\frac{a_i+a_{i+1}}{2}$ is the midpoint between the values of $a_i$ and $a_{i+1}$.
          \end{itemize}
          \item The point with the minimum expected information requirement for $A$ \\ is selected as the split point for $A$.
        \end{itemize}
        \item \textbf{Split:}
        \begin{itemize}
          \item $D_1$ is the set of tuples in $D$ satisfying $A \leq$ split point,\\
                and $D_2$ is the set of tuples in $D$ satisfying $A >$ split point.
        \end{itemize}
        \item \textbf{So to say: Discretization as you go along.}
        \begin{itemize}
          \item For this particular purpose.
        \end{itemize}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Gain ratio for attribute selection (C4.5)}
      \begin{itemize}
        \item \textbf{Information-gain measure is biased towards attributes with a large number of values.}
        \item \textbf{C4.5 (a successor of ID3) uses gain ratio to overcome the problem (normalization to information gain):}
        \begin{align}
          \text{SplitInfo}_A(D) = - \sum_{j=1}^{v} \frac{|D_j|}{|D|} \log_2\left( \frac{|D_j|}{|D|} \right),\\
          \text{GainRatio}(A) = \frac{\text{Gain}(A)}{\text{SplitInfo}_A(D)}.
        \end{align}
        \item Example:
        \begin{align}
          \text{SplitInfo}_{\texttt{income}}(D) &= -\frac{4}{14} \log_2 \left( \frac{4}{14} \right) - \frac{6}{14} \log_2 \left( \frac{6}{14} \right) - \frac{4}{14} \log_2 \left( \frac{4}{14} \right) = 1.557,\\
          \text{GainRatio}(\texttt{income}) &= \frac{0.029}{1.557} = 0.019.
        \end{align}
        \item The attribute with the maximum gain ratio is selected as the splitting attribute.
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Gini index}
      \begin{itemize}
        \item \textbf{Corrado Gini (1884 -- 1965).}
        \begin{itemize}
          \item Italian statistician and sociologist.
        \end{itemize}
        \item \textbf{Also called Gini coefficient.}
        \item \textbf{Measures statistical dispersion.}
        \begin{itemize}
          \item Zero expresses perfect equality where all values are the same.
          \item One expresses maximal inequality among values.
        \end{itemize}
        \item \textbf{Based on the Lorenz curve.}
        \begin{itemize}
          \item Plots the proportion of the total sum of values ($y$-axis) that is cumulatively assigned to the bottom $x\%$ of the population.
          \item Line at $45$ degrees thus represents perfect equality of value distribution.
        \end{itemize}
        \item \textbf{Gini coefficient then is $\ldots$}
        \begin{itemize}
          \item $\ldots$ the ratio of the area that lies between the line of equality and the Lorenz curve over the total area under the line of equality.
        \end{itemize}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Gini index (II)}
      \centering
      Example: Distribution of incomes.\\[0.5cm]
      \begin{tikzpicture}
      \begin{axis}[
          xmin=0, xmax=100,
          ymin=0, ymax=100,
          minor tick num = 4,
          grid,
          ylabel = cumulative income (in \%),
          xlabel = cumulative population (in \%),
          legend style={legend pos=north west},
          ]
      \addplot plot
          coordinates {(0,0) (25,20) (50,15) (75,30) (100,100)};
      \addplot plot
          coordinates {(0,0) (25,10) (50,45) (75,40) (100,100)};
      \addplot plot [thin]
          coordinates {(0,0) (100,100)};
      \legend{$L(1)$,$L(2)$}
      \end{axis}
    \end{tikzpicture}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Gini index (CART, IBM IntelligentMiner)}
      \begin{itemize}
        \item \textbf{If a dataset D contains examples from n classes, Gini index gini(D) is defined as:}
        \begin{align}
          \text{gini}(D) = 1-\sum_{j=1}^{n} p_j^2,
        \end{align}
        where $p_j$ is the relative frequency of class $j$ in $D$.
        \item \textbf{If a dataset $D$ is split on $A$ into two subsets $D_1$ and $D_2$, the Gini index $\text{gini}(D)$ is defined as}
        \begin{align}
          \text{gini}_A(D) = \frac{|D_1|}{|D|}\text{gini}(D_1)+\frac{|D_2|}{|D|}\text{gini}(D_2).
        \end{align}
        \item \textbf{Reduction in impurity:}
        \begin{align}
          \Delta \text{gini}_A(A) = \text{gini}(D)-\text{gini}_A(D).
        \end{align}
        \item \textbf{The attribute $A$ provides the smallest $\text{gini}_A(D)$ (or the largest reduction in impurity) \\ is chosen to split the node.}
        \begin{itemize}
          \item Need to enumerate all the possible splitting points for each attribute.
        \end{itemize}
      \end{itemize}
    \end{frame}
  }


  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Computation of Gini index}
      \begin{itemize}
        \item \textbf{Example:}
        \begin{itemize}
          \item $D$ has $9$ tuples in $\text{buys\_computer} =$ "yes" and 5 in "no", thus
          \begin{align}
            \text{gini}(D) = 1 - \left( \frac{9}{14} \right)^2 - \left( \frac{5}{14} \right) = 0.459.
          \end{align}
        \end{itemize}
        \item Suppose the attribute $\texttt{income}$ partitions $D$ \\ into $10$ in $D_1:\{\texttt{low,medium}\}$ and $4$ in $D_2: \{\texttt{high}\}$:
        \begin{align}
          &\text{gini}(D\vert_{D[\texttt{income}]="medium", "low"}) \\
          &= \left( \frac{10}{14} \right) \text{gini}(D_1) + \frac{4}{14} \text{gini}(D_2)\\
          &=\frac{10}{14} \left(1-\left( \frac{7}{10} \right)^2 - \left( \frac{3}{10} \right)^2 \right) + \frac{4}{14} \left( 1-\left( \frac{2}{4} \right)^2 - \left( \frac{2}{4} \right)^2 \right) =\\
          &= 0.443 = \text{gini}(D\vert_{D[\texttt{income}]="high"}).
        \end{align}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Computation of Gini index (II)}
      \begin{itemize}
        \item \textbf{Example (cont.):}
        \begin{itemize}
          \item $\text{gini}(D\vert_{D[\texttt{income}]="low", "high"}) = 0.458$,\\
                $\text{gini}(D\vert_{D[\texttt{income}]="medium", "high"}) = 0.450.$
          \item Thus, split on the \{"low","medium"\} and \{"high"\}, since it has the lowest gini index.
        \end{itemize}
        \item \textbf{All attributes are assumed continuous-valued.}
        \item \textbf{May need other tools, e.g. clustering, to get the possible split values.}
        \item \textbf{Can be modified for categorical attributes.}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Computation of Gini index (II)}
      \begin{itemize}
        \item \textbf{The three measures, in general, return good results, but}
        \begin{itemize}
          \item \textbf{\color{airforceblue}Information gain:}
          \begin{itemize}
            \item Biased towards multi-valued attributes.
          \end{itemize}
          \item \textbf{\color{airforceblue}Gain ratio:}
          \begin{itemize}
            \item Tends to prefer unbalanced splits in which one partition is much smaller than the others.
          \end{itemize}
          \item \textbf{\color{airforceblue}Gini index:}
          \begin{itemize}
            \item Biased to multi-valued attributes.
            \item Has difficulty when number of classes is large.
            \item Tends to favor tests that result in equal-sized partitions and purity in both partitions.
          \end{itemize}
        \end{itemize}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Other attribute-selection measures}
      \begin{itemize}
        \item \textbf{CHAID:}
        \begin{itemize}
          \item A popular decision-tree algorithm, measure based on $\chi^2$ test for independence.
        \end{itemize}
        \item \textbf{C-SEP:}
        \begin{itemize}
          \item Performs better than information gain and Gini index in certain cases.
        \end{itemize}
        \item \textbf{G-statistic:}
        \begin{itemize}
          \item Has a close approximation to $\chi^2$ distribution.
        \end{itemize}
        \item \textbf{MDL (Minimal Description Length) principle:}
        \begin{itemize}
          \item I.e. the simplest solution is preferred.
          \item The best tree is the one that requires the fewest number of bits to both (1) encode the tree and (2) encode the exceptions to the tree.
        \end{itemize}
        \item \textbf{Multivariate splits:}
        \begin{itemize}
          \item Partitioning based on multiple variable combinations.
          \item CART: finds multivariate splits based on a linear combination of attributes.
        \end{itemize}
        \item \textbf{Which attribute-selection measure is the best?}
        \begin{itemize}
          \item Most give good results, none is significantly superior to others.
        \end{itemize}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Overfitting and tree pruning}
      \begin{itemize}
        \item \textbf{Overfitting: An induced tree may overfit the training data.}
        \begin{itemize}
          \item Too many branches, some may reflect anomalies due to noise or outliers.
          \item Poor accuracy for unseen samples.
        \end{itemize}
        \item \textbf{Two approaches to avoid overfitting:}
        \begin{itemize}
          \item \textbf{\color{airforceblue}Prepruning:}
          \begin{itemize}
            \item Halt tree construction early.\\
                  Do not split a node, if this would result in the goodness measure falling below a threshold.
            \item Difficult to choose an appropriate threshold.
          \end{itemize}
          \item \textbf{\color{airforceblue}Postpruning:}
          \begin{itemize}
            \item Remove branches from a "fully grown" tree.\\
                  Get a sequence of progressively pruned trees.
            \item Use a set of data different from the training data to decide which is the "best pruned tree."
          \end{itemize}
        \end{itemize}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Enhancements to Basic Decision-Tree Induction}
      \begin{itemize}
        \item \textbf{Allow for} \textbf{\color{airforceblue}continuous-valued attributes.}
        \begin{itemize}
          \item Dynamically define new discrete-valued attributes that partition the values of continuous-valued attributes into a discrete set of intervals.
        \end{itemize}
        \item \textbf{Handle} \textbf{\color{airforceblue}missing attribute values.}
        \begin{itemize}
          \item Assign the most common value of the attribute.
          \item Assign probability to each of the possible values.
        \end{itemize}
      \item \textbf{\color{airforceblue}Attribute construction.}
      \begin{itemize}
        \item Create new attributes based on existing ones that are sparsely represented.
        \item This reduces fragmentation, repetition, and replication.
      \end{itemize}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Classification in large databases}
      \begin{itemize}
        \item \textbf{Classification -- a classical problem extensively \\ studied by statisticians and machine-learning researchers.}
        \item \textbf{Scalability:}
        \begin{itemize}
          \item Classifying datasets with millions of examples and \\ hundreds of attributes with reasonable speed.
        \end{itemize}
        \item \textbf{Why is decision-tree induction popular?}
        \begin{itemize}
          \item Relatively fast learning speed (compared to other classification methods).
          \item Convertible to simple and easy-to-understand classification rules.
          \item Can use SQL queries for accessing databases.
          \item Classification accuracy comparable with other methods.
        \end{itemize}
        \item \textbf{RainForest (Gehrke, Ramakrishnan \& Ganti, VLDB'98).}
        \begin{itemize}
          \item Builds an AVC-list (attribute, value, class label).
        \end{itemize}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Scalability framework for RainForest}
      \begin{itemize}
        \item \textbf{Separates the scalability aspects from the criteria that determine the quality of the tree.}
        \item \textbf{Builds an} \textbf{\color{airforceblue}AVC-list:}.
        \begin{itemize}
          \item AVC (Attribute, Value, Class\_label).
        \end{itemize}
        \item \textbf{\color{airforceblue}AVC-set} \textbf{(of an attribute X):}
        \begin{itemize}
          \item Projection of training dataset onto the attribute $X$ and class label where counts of individual class label are aggregated.
        \end{itemize}
        \item \textbf{\color{airforceblue}AVC-group} \textbf{(of a node n):}
        \begin{itemize}
          \item Set of AVC-sets of all predictor attributes at the node $n$.
        \end{itemize}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{RainForest: training set and its AVC-sets}
      \begin{columns}
        \begin{column}{0.6\textwidth}
          \begin{tabular}{|c|l|c|l|c|}
            \hline
            \cellcolor{blue!20}age & \cellcolor{blue!20}income & \cellcolor{blue!20}student & \cellcolor{blue!20}credit\_rating & \cellcolor{brown!20}buys\_computer \\\hline
            \cellcolor{yellow!20}$\leq30$ & \cellcolor{yellow!20}high & \cellcolor{yellow!20}no & \cellcolor{yellow!20}fair & \cellcolor{red!20}no \\\hline
            \cellcolor{yellow!20}$\leq30$ & \cellcolor{yellow!20}high & \cellcolor{yellow!20}no & \cellcolor{yellow!20}excellent & \cellcolor{red!20}no \\\hline
            \cellcolor{yellow!20}$31\ldots40$ & \cellcolor{yellow!20}high & \cellcolor{yellow!20}no & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$>40$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}no & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$>40$ & \cellcolor{yellow!20}low & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$>40$ & \cellcolor{yellow!20}low & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}excellent & \cellcolor{red!20}no \\\hline
            \cellcolor{yellow!20}$31\ldots40$ & \cellcolor{yellow!20}low & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}excellent & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$\leq30$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}no & \cellcolor{yellow!20}fair & \cellcolor{red!20}no \\\hline
            \cellcolor{yellow!20}$\leq30$ & \cellcolor{yellow!20}low & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$>40$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$\leq30$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}excellent & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$31\ldots40$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}no & \cellcolor{yellow!20}excellent & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$31\ldots40$ & \cellcolor{yellow!20}high & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$>40$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}no & \cellcolor{yellow!20}excellent & \cellcolor{red!20}no \\\hline
          \end{tabular}
        \end{column}
        \begin{column}{0.3\textwidth}
          \vspace{-3cm}

          \centering
          AVC-set on age:\\
          \begin{tabular}{|c|c|c|}
            \hline
            age & yes & no \\\hline
            $\leq 30$ & 2 & 3 \\\hline
            $31\ldots40$ & 4 & 0 \\\hline
            $>40$ & 3 & 2 \\\hline
          \end{tabular}\\[1cm]
          AVC-set on income:\\
          \begin{tabular}{|c|c|c|}
            \hline
            income & yes & no \\\hline
            high & 2 & 2 \\\hline
            medium & 4 & 2 \\\hline
            low & 3 & 1 \\\hline
          \end{tabular}
        \end{column}
      \end{columns}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{RainForest: training set and its AVC-sets (II)}
      \begin{columns}
        \begin{column}{0.6\textwidth}
          \begin{tabular}{|c|l|c|l|c|}
            \hline
            \cellcolor{blue!20}age & \cellcolor{blue!20}income & \cellcolor{blue!20}student & \cellcolor{blue!20}credit\_rating & \cellcolor{brown!20}buys\_computer \\\hline
            \cellcolor{yellow!20}$\leq30$ & \cellcolor{yellow!20}high & \cellcolor{yellow!20}no & \cellcolor{yellow!20}fair & \cellcolor{red!20}no \\\hline
            \cellcolor{yellow!20}$\leq30$ & \cellcolor{yellow!20}high & \cellcolor{yellow!20}no & \cellcolor{yellow!20}excellent & \cellcolor{red!20}no \\\hline
            \cellcolor{yellow!20}$31\ldots40$ & \cellcolor{yellow!20}high & \cellcolor{yellow!20}no & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$>40$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}no & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$>40$ & \cellcolor{yellow!20}low & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$>40$ & \cellcolor{yellow!20}low & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}excellent & \cellcolor{red!20}no \\\hline
            \cellcolor{yellow!20}$31\ldots40$ & \cellcolor{yellow!20}low & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}excellent & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$\leq30$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}no & \cellcolor{yellow!20}fair & \cellcolor{red!20}no \\\hline
            \cellcolor{yellow!20}$\leq30$ & \cellcolor{yellow!20}low & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$>40$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$\leq30$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}excellent & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$31\ldots40$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}no & \cellcolor{yellow!20}excellent & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$31\ldots40$ & \cellcolor{yellow!20}high & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$>40$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}no & \cellcolor{yellow!20}excellent & \cellcolor{red!20}no \\\hline
          \end{tabular}
        \end{column}
        \begin{column}{0.3\textwidth}
          \vspace{-3cm}

          \centering
          AVC-set on student:\\
          \begin{tabular}{|c|c|c|}
            \hline
            student & yes & no \\\hline
            yes & 6 & 1 \\\hline
            no & 3 & 4 \\\hline
          \end{tabular}\\[1cm]
          AVC-set on credit\_rating:\\
          \begin{tabular}{|c|c|c|}
            \hline
            credit\_rating & yes & no \\\hline
            fair & 6 & 2 \\\hline
            excellent & 3 & 3 \\\hline
          \end{tabular}
        \end{column}
      \end{columns}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{BOAT (Bootstrapped optimistic algorithm for tree construction)}
      \begin{itemize}
        \item \textbf{Use a statistical technique called bootstrapping to create several smaller samples (subsets), each fitting in memory.}
        \begin{itemize}
          \item See on the subsequent slides.
        \end{itemize}
        \item \textbf{Each subset is used to create a tree, resulting in several trees.}
        \item \textbf{These trees are examined and used to construct a new tree T'.}
        \begin{itemize}
          \item It turns out that T' is very close to the tree that would be generated \\
          using the whole data set together.
        \end{itemize}
        \item \textbf{Advantages:}
        \begin{itemize}
          \item Requires only two scans of DB.
          \item An incremental algorithm:
          \begin{itemize}
            \item Take insertions and deletions of training data and update the decision tree.
          \end{itemize}
        \end{itemize}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Presentation of classification results}
      \centering
      \includegraphics[height=0.9\textheight]{img/classification1.jpeg}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Visualization of a decision tree in SGI/MineSet 3.0}
      \centering
      \includegraphics[height=0.9\textheight]{img/classification2.jpeg}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Interactive visual mining by perception-based classification (PBC)}
      \centering
      \includegraphics[height=0.9\textheight]{img/classification3.jpeg}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Chapter VI: Classification}
        \begin{itemize}
            \item Classification: basic concepts.
            \item Decision-tree induction.
            \item \textbf{Bayes classification methods.}
            \item Rule-based classification.
            \item Model evaluation and selection.
            \item Techniques to improve classification accuracy: ensemble methods.
            \item Summary.
        \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Bayesian classification: why?}
        \begin{itemize}
            \item \textbf{A statistical classifier:}
            \begin{itemize}
              \item Performs probabilistic prediction, i.e. predicts class-membership probabilities.
            \end{itemize}
            \item \textbf{Foundation:} \textbf{\color{airforceblue}Bayes' Theorem.}
            \item \textbf{Performance:}
            \begin{itemize}
              \item A simple Bayesian classifier (naÃ¯ve Bayesian classifier) has performance comparable with decision tree and selected neural-network classifiers.
            \end{itemize}
            \item \textbf{Incremental:}
            \begin{itemize}
              \item Each training example can incrementally increase/decrease the probability that a hypothesis is correct--prior knowledge can be combined with observed data.
            \end{itemize}
            \item \textbf{Standard:}
            \begin{itemize}
              \item Even when Bayesian methods are computationally intractable, they can provide a standard of optimal decision making against which other methods can be measured.
            \end{itemize}
        \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Bayes' Theorem: basics}
        \begin{itemize}
          \item \textbf{Let $X$ be a data sample ("evidence").}
          \begin{itemize}
            \item The class label shall be unknown.
          \end{itemize}
          \item \textbf{Let $C_i$ be the hypothesis that $X$ belongs to class $i$.}
          \item \textbf{Classification is to determine $P(C_i|X)$:}
          \begin{itemize}
            \item \textbf{\color{airforceblue}Posteriori probability:} the probability that the hypothesis \\ holds given the observed data sample $X$.
          \end{itemize}
          \item $P(C_i)$:
          \begin{itemize}
            \item \textbf{\color{airforceblue}Prior probability:} the initial probability.
            \item E.g. $X$ will buy computer, regardless of age, income, $\ldots$
          \end{itemize}
          \item $P(X)$:
          \begin{itemize}
            \item Probability that sample data is observed.
          \end{itemize}
          \item $P(X|C_j)$:
          \begin{itemize}
            \item \textbf{\color{airforceblue}Likelihood:} the probability of observing the sample $X$ given that the hypothesis holds.
            \item E.g. given that $X$ buys computer, the probability that $X$ is $31\ldots40$, medium income.
          \end{itemize}
        \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Bayes' Theorem (II)}
        \begin{itemize}
          \item \textbf{Given training data $X$, the posteriori probability $P(C_i|X)$\\
           of a hypothesis $C_i$ follows from the Bayes' Theorem:}
           \begin{align}
             P(C_i|X) = \frac{P(X|C_i)P(C_i)}{P(X)}.
           \end{align}
          \item \textbf{Predicts that $X$ belongs to $C_i$ iff the probability $P(C_i|X)$\\
           is {\color{airforceblue}the highest} among all the $P(C_k|X)$ for all $k$ classes.}
           \item \textbf{Practical difficulty:}
           \begin{itemize}
             \item Requires initial knowledge of many probabilities.
             \item Significant computational cost.
           \end{itemize}
        \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Towards naÃ¯ve Bayesian classifier}
        \begin{itemize}
          \item \textbf{Let $D$ be a training set of tuples and their associated class labels.}
          \begin{itemize}
            \item Each tuple is represented by an $n$-dimensional attribute $X = (x_1,x_2,\ldots,x_n)$.
          \end{itemize}
          \item \textbf{Supose there are $m$ classes $C_1,C_2, \ldots, C_m$.}
          \item \textbf{Classification is to derive the {\color{airforceblue}maximum posteriori probability}.}
          \begin{itemize}
            \item i.e. the maximal $P(C_i|X)$.
          \end{itemize}
          \item \textbf{This can be derived from Bayes' Theorem:}
          \begin{align}
            P(C_i|X) = \frac{P(X|C_i)P(C_i)}{P(X)}.
          \end{align}
          \item \textbf{Since $P(X)$ is constant for all classes, we must maximize only:}
          \begin{align}
            P(X|C_i)P(C_i).
          \end{align}
        \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Derivation of naÃ¯ve Bayes classifier}
        \begin{itemize}
          \item \textbf{A simplifying assumption: attributes are {\color{airforceblue}conditionally independent}.}
          \begin{itemize}
            \item I.e. no dependence relation between attributes (which is "naÃ¯ve").
            \begin{align*}
              \resizebox{7cm}{!}{%
              $P(X|C_i) = \prod_{k=1}^{n} P(x_k|C_i) = P(x_1|C_i)P(x_2|C_i)\cdots P(x_k|C_i).$}
            \end{align*}
            \item This greatly reduces the computation cost:\\
                  Only count the class distribution.
            \item If $A_k$ is categorical,
            \begin{itemize}
              \item $P(x_k|C_i)$is the number of tuples in $C_i$ having value $x_k$ for $A_k$ \\
                    divided by $|C_{i,D}|$ (the number of tuples of $C_i$ in $D$).
            \end{itemize}
            \item If $A_k$ is continuous-valued,
            \begin{itemize}
              \item $P(x_k|C_i)$ is usually computed based on Gaussian distribution with a mean $\mu$ and standard deviation $\sigma$:
              \begin{align*}
                \resizebox{4cm}{!}{%
                $G(x,\mu,\sigma) = \frac{1}{\sqrt{2\pi}\sigma}e^{\frac{(x-\mu)^2}{2\sigma^2}},$}
              \end{align*}
              \item and $P(x_k|C_i)$ is $P(x_k|C_i) = G(x_k,\mu_{C_i},\sigma_{C_i})$.
            \end{itemize}
          \end{itemize}
        \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{NaÃ¯ve Bayesian cellcolormake dataset}
      \begin{columns}
        \begin{column}{0.4\textwidth}
          \vspace{-2cm}
          \begin{itemize}
            \item \textbf{Classes:}
            \begin{itemize}
              \item $C_1$: \texttt{buys\_computer} = "yes".
              \item $C_2$: \texttt{buys\_computer} = "no".
            \end{itemize}
            \item \textbf{Data sample:}
            \begin{itemize}
              \item $X = (\texttt{age} \leq 30,$ \\
              $\texttt{income} = "medium",$ \\
              $\texttt{student} = "yes",$\\
              $\texttt{credit\_rating} = "fair")$.
            \end{itemize}
          \end{itemize}
        \end{column}
        \begin{column}{0.6\textwidth}
          \resizebox{\columnwidth}{!}{%
          \begin{tabular}{|l|l|c|l|c|}
            \hline
            \cellcolor{blue!20}age & \cellcolor{blue!20}income & \cellcolor{blue!20}student & \cellcolor{blue!20}credit\_rating & \cellcolor{brown!20}buys\_computer \\\hline
            \cellcolor{yellow!20}$\leq30$ & \cellcolor{yellow!20}high & \cellcolor{yellow!20}no & \cellcolor{yellow!20}fair & \cellcolor{red!20}no \\\hline
            \cellcolor{yellow!20}$\leq30$ & \cellcolor{yellow!20}high & \cellcolor{yellow!20}no & \cellcolor{yellow!20}excellent & \cellcolor{red!20}no \\\hline
            \cellcolor{yellow!20}$31\ldots40$ & \cellcolor{yellow!20}high & \cellcolor{yellow!20}no & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$>40$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}no & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$>40$ & \cellcolor{yellow!20}low & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$>40$ & \cellcolor{yellow!20}low & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}excellent & \cellcolor{red!20}no \\\hline
            \cellcolor{yellow!20}$31\ldots40$ & \cellcolor{yellow!20}low & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}excellent & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$\leq30$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}no & \cellcolor{yellow!20}fair & \cellcolor{red!20}no \\\hline
            \cellcolor{yellow!20}$>40$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$\leq30$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}excellent & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$31\ldots40$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}no & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$31\ldots40$ & \cellcolor{yellow!20}high & \cellcolor{yellow!20}yes & \cellcolor{yellow!20}fair & \cellcolor{green!20}yes \\\hline
            \cellcolor{yellow!20}$>40$ & \cellcolor{yellow!20}medium & \cellcolor{yellow!20}no & \cellcolor{yellow!20}excellent & \cellcolor{red!20}no \\\hline
          \end{tabular}}
        \end{column}
      \end{columns}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{NaÃ¯ve Bayesian classifier: an example}
      \begin{itemize}
        \item $P(C_i)$:
        \begin{itemize}
          \item $P(\texttt{buys\_computer} = "yes") = \frac{9}{14} = 0.643$.
          \item $P(\texttt{buys\_computer} = "no") = \frac{5}{14} = 0.357$.
        \end{itemize}
        \item $X = (\texttt{age} \leq 30 , \texttt{income} = "medium", \texttt{student} = "yes", \texttt{credit\_rating} = "fair")$.
        \item \textbf{Compute $P(X|C_i)$ for each class:}
        \begin{itemize}
          \item $P(\texttt{age} \leq 30 | \texttt{buys\_computer} = "yes") = \frac{2}{9} = 0.222$.
          \item $P(\texttt{age} \leq 30 | \texttt{buys\_computer} = "no") = \frac{3}{5} = 0.6$.
          \item $P(\texttt{income} = "medium" | \texttt{buys\_computer} = "yes") = \frac{4}{9} = 0.444$.
          \item $P(\texttt{income} = "medium" | \texttt{buys\_computer} = "no") = \frac{2}{5} = 0.4$.
          \item $P(\texttt{student} = "yes" | \texttt{buys\_computer} = "yes") = \frac{6}{9} = 0.667$.
          \item $P(\texttt{student} = "yes" | \texttt{buys\_computer} = "no") = \frac{1}{5} = 0.2$.
          \item $P(\texttt{credit\_rating} = "fair" | \texttt{buys\_computer} = "yes") = \frac{6}{9} = 0.667$.
          \item $P(\texttt{credit\_rating} = "fair" | \texttt{buys\_computer} = "no") = \frac{2}{5} = 0.4$.
        \end{itemize}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{NaÃ¯ve Bayesian classifier: an example (II)}
      \begin{itemize}
        \item $P(C_i)$:
        \begin{itemize}
          \item $P(X | \texttt{buys\_computer} = "yes") = 0.222 \cdot 0.444 \cdot 0.667 \cdot 0.667 = 0.044$.
          \item $P(X | \texttt{buys\_computer} = "no") = 0.6 \cdot 0.4 \cdot 0.2 \cdot 0.4 = 0.019$.
        \end{itemize}
        \item $P(X | C_i) \cdot P(C_i)$:
        \begin{itemize}
          \item $P(X | \texttt{buys\_computer} = "yes") \cdot  P(\texttt{buys\_computer} = "yes") = 0.028$.
          \item $P(X | \texttt{buys\_computer} = "no") \cdot  P(\texttt{buys\_computer} = "no") = 0.007$.
        \end{itemize}
        \item \textbf{Therefore, $X$ belongs to class $C_1$ (\texttt{buys\_computer} = "yes")}.
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Avoiding the zero-probability problem}
      \begin{itemize}
        \item \textbf{NaÃ¯ve Bayesian prediction requires each conditional probability to be non-zero.}
        \begin{itemize}
          \item Otherwise, the predicted probability will be zero.
          \begin{align}
            \resizebox{4cm}{!}{
            $P(X|C_i) = \prod_{k=1}^{n} P(x_k|C_i).$}
          \end{align}
        \end{itemize}
        \item \textbf{Example:}
        \begin{itemize}
          \item Suppose a dataset with $1000$ tuples, \texttt{income} = "low" $(0)$, \texttt{income} = "medium" $(990)$, and \texttt{income} = "high" $(10)$.
        \end{itemize}
        \item \textbf{Use {\color{airforceblue}Laplacian correction} (or Laplacian estimator):}
        \begin{itemize}
          \item Add $1$ to each case:
          \begin{itemize}
            \item $P(\texttt{income} = "low") = \frac{1}{1003}$.
            \item $P(\texttt{income} = "medium") = \frac{991}{1003}$.
            \item $P(\texttt{income} = "high") = \frac{11}{1003}$.
          \end{itemize}
          \item The "corrected" probability estimates are close to their "uncorrected" counterparts.
        \end{itemize}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{NaÃ¯ve Bayesian classifier: comments}
      \begin{itemize}
        \item \textbf{Advantages:}
        \begin{itemize}
          \item Easy to implement.
          \item Good results obtained in most of the cases.
        \end{itemize}
        \item \textbf{Disadvantages:}
        \begin{itemize}
          \item Assumption: class conditional independence, therefore loss of accuracy.
          \item Practically, \textbf{dependencies} exist among variables.
          \begin{itemize}
            \item E.g. hospital patients:
            \begin{itemize}
              \item Profile: age, family history, etc.
              \item Symptoms: fever, cough, etc.
              \item Disease: lung cancer, diabetes, etc.
            \end{itemize}
            \item Cannot be modeled by naÃ¯ve Bayesian classifier.
          \end{itemize}
        \end{itemize}
      \item \textbf{How to deal with these dependencies?}
      \begin{itemize}
        \item Bayesian belief networks (see textbook).
      \end{itemize}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Chapter VI: Classification}
        \begin{itemize}
            \item Classification: basic concepts.
            \item Decision-tree induction.
            \item Bayes classification methods.
            \item \textbf{Rule-based classification.}
            \item Model evaluation and selection.
            \item Techniques to improve classification accuracy: ensemble methods.
            \item Summary.
        \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Using \uppercase{if-then} rules for classification}
      \begin{itemize}
        \item \textbf{Represent the knowledge in the form of {\color{airforceblue}IF-THEN rules}.}
        \begin{itemize}
          \item E.g. if \texttt{age} $\leq 30$ AND \texttt{student} = "yes" THEN buys\_computer = "yes".
          \item Readable.
        \end{itemize}
        \item \textbf{Rule {\color{airforceblue}antecedent/precondition} vs. rule {\color{airforceblue}consequent}}.
        \item \textbf{Assessment of a rule R: coverage and accuracy.}
        \begin{itemize}
          \item $n_{\text{covers}} = \#$ of tuples covered by $R$ (antecedent if true).
          \item $n_{\text{correct}} = \#$ of tuples correctly classified by $R$.
          \item $\text{coverage}(R) = \frac{n_{\text{covers}}}{|D|}$ with $D$ training data set.
          \item $\text{accuracy}(R) = \frac{n_{\text{correct}}}{n_{\text{covers}}}$.
        \end{itemize}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Using \uppercase{if-then} rules for classification (II)}
      \begin{itemize}
        \item \textbf{If more than one rule are triggered, need {\color{airforceblue}conflict resolution}.}
        \begin{itemize}
          \item \textbf{\color{airforceblue}Size ordering:}
          \begin{itemize}
            \item Assign the highest priority to the triggered rule that has the "toughest" requirement \\ (i.e., the most attribute tests).
          \end{itemize}
          \item \textbf{\color{airforceblue}Class-based ordering:}
          \begin{itemize}
            \item Decreasing order of prevalence or misclassification cost per class.
          \end{itemize}
        \item \textbf{\color{airforceblue}Rule-based ordering} (decision list):
        \begin{itemize}
          \item Rules are organized into one long priority list,\\
          according to some measure of rule quality, or by experts.
        \end{itemize}
      \end{itemize}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Rule extraction from a decision tree}
      \begin{itemize}
        \item \textbf{Rules are {\color{airforceblue}easier to understand} than large trees.}
        \item \textbf{Rule can be created for {\color{airforceblue}each path from the root to a leaf.}}
        \begin{itemize}
          \item The leaf holds the class prediction.
        \end{itemize}
        \item \textbf{Each attribute-value pair along the path forms a conjunction:}
      \end{itemize}
      \begin{columns}
        \begin{column}{0.45\textwidth}
          \vspace{-3.3cm}
          \begin{itemize}
            \item \textbf{Example:}
            \begin{itemize}
              \item IF \texttt{age} $\leq$ 30 AND \texttt{student} = "no" \\
                    THEN \texttt{buys\_computer} = "no".
              \item IF \texttt{age} $\leq$ 30 AND \texttt{student} = "yes" \\
                    THEN \texttt{buys\_computer} = "yes".
              \item IF \texttt{age} $31\ldots40$ THEN \texttt{buys\_computer} = "yes".
              \item
              \item
            \end{itemize}
          \end{itemize}
        \end{column}
        \begin{column}{0.55\textwidth}
          \centering
          \begin{tikzpicture}
            \node[draw, fill=blue!20] at (0,0) (a) {age?};
            \node[fill=yellow!20] at (-1.5,-0.7) (b) {<=30};
            \node[fill=yellow!20] at (0,-0.7) (c) {$31\ldots40$};
            \node[fill=yellow!20] at (1.5,-0.7) (d) {$>40$};
            \node[draw, fill=blue!20] at (-1.5,-1.4) (e) {student?};
            \node[fill=yellow!20] at (-2,-2.1) (eno1) {no};
            \node[fill=yellow!20] at (-1,-2.1) (eyes1) {yes};
            \node[fill=green!20] at (-2,-2.8) (eno2) {no};
            \node[fill=green!20] at (-1,-2.8) (eyes2) {yes};
            \node[draw, fill=blue!20] at (1.5,-1.4) (g) {credit rating?};
            \node[fill=yellow!20] at (2,-2.1) (gf) {fair};
            \node[fill=yellow!20] at (1,-2.1) (gex) {excellent};
            \node[fill=orange!20] at (1,-2.8) (gno) {no};
            \node[fill=green!20] at (2,-2.8) (gyes) {yes};
            \node[fill=green!20] at (0,-1.4) (f) {yes};

            \draw (a)--(b);
            \draw (a)--(c);
            \draw (a)--(d);
            \draw (b)--(e);
            \draw (c)--(f);
            \draw (d)--(g);
            \draw (e)--(eno1);
            \draw (e)--(eyes1);
            \draw (eno1)--(eno2);
            \draw (eyes1)--(eyes2);
            \draw (g)--(gex);
            \draw (g)--(gf);
            \draw (gex)--(gno);
            \draw (gf)--(gyes);
          \end{tikzpicture}
        \end{column}
      \end{columns}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Rule induction: sequential covering method}
      \begin{itemize}
        \item \textbf{Sequential covering algorithm:}
        \begin{itemize}
          \item Extracts rules directly from training data.
        \end{itemize}
        \item \textbf{Typical sequential covering algorithms:}
        \begin{itemize}
          \item FOIL, AQ, CN2, RIPPER.
        \end{itemize}
        \item \textbf{Rules are learned {\color{airforceblue}sequentially}.}
        \begin{itemize}
          \item Each rule for a given class $C_i$ will cover many tuples of $C_i$, but none (or few) of the tuples of other classes.
        \end{itemize}
        \item \textbf{Steps:}
        \begin{itemize}
          \item Rules are learned one at a time.
          \item Each time a rule is learned, the tuples covered by the rule are removed.
          \item The process repeats on the remaining tuples unless termination condition, e.g. when no more training examples left or when the quality of a rule returned is below a user-specified threshold.
        \end{itemize}
        \item \textbf{Compare with decision-tree induction:}
        \begin{itemize}
          \item That was learning a set of rules simultaneously.
        \end{itemize}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Sequential covering algorithm}
      \begin{itemize}
        \item \textbf{While (enough target tuples left):}
        \begin{itemize}
          \item generate a rule;
          \item remove positive target tuples satisfying this rule;
        \end{itemize}
        \centering
        \begin{tikzpicture}
          \draw[fill=airforceblue] (0,0) ellipse (5.2 and 2.5) (0,0) node [text=black] {};
          \draw[fill=white] (0,1) ellipse (2 and 1.2) (0,0) node [text=black] {};
          \draw[fill=white] (-3,-0.2) ellipse (2 and 1.2) (0,0) node [text=black] {};
          \draw[fill=white] (3,0) ellipse (2 and 1.2) (0,0) node [text=black] {};
          \node at (0,1.1) (a1) {Examples covered};
          \node at (0,0.8) (a2) {by rule 2};
          \node at (3,0.1) (b1) {Examples covered};
          \node at (3,-0.2) (b2) {by rule 3};
          \node at (-3,-0.2) (c1) {Examples covered};
          \node at (-3,-0.5) (c2) {by rule 1};
          \node at (-1,-1.5) (d1) {\textbf{Positive}};
          \node at (-1,-1.8) (d2) {\textbf{examples}};
        \end{tikzpicture}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Sequential covering algorithm}
      \begin{itemize}
        \item \textbf{To generate a rule:}
        \begin{itemize}
          \item \textbf{while}(true:)
          \begin{itemize}
            \item find the best predicate $p$ (attribute = value);
            \item \textbf{if} \texttt{FOIL\_Gain}(p) > threshold
            \item \textbf{then} add $p$ to current rule;
            \item \textbf{else} break;
          \end{itemize}
        \end{itemize}
      \end{itemize}
      \centering
      \begin{tikzpicture}
        \draw[fill=red!20] (-3,0) rectangle (1,4) (0,0) node [text=black] {};
        \draw[fill=airforceblue!20] (6,0) rectangle (0,4) (0,0) node [text=black] {};
        \node at (3,0.5) (a1) {Negative examples};
        \node at (-1.5,0.5) (a1) {Positive examples};
        \draw[fill=yellow!20, opacity=0.5] (-0.7,2.5) ellipse (2 and 1.2) (0,0) node [text=black] {};
        \node[opacity=0.5] at (-0.7,3) {$A3=1$};
        \draw[fill=yellow!20, opacity=0.5] (-0.7,2.5) ellipse (1.8 and 1) (0,0) node [text=black] {};
        \node[opacity=0.5] at (-0.7,2.7) {$A3=1 \&\& A1=2$};
        \draw[fill=yellow!20, opacity=0.5] (-0.7,2.5) ellipse (1.6 and 0.8) (0,0) node [text=black] {};
        \node[opacity=0.5] at (-0.7,2.4) {$A3=1 \&\& A1=2$};
        \node[opacity=0.5] at (-0.7,2.1) {$\&\& A8=5$};
      \end{tikzpicture}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Sequential covering algorithm}
      \begin{itemize}
        \item \textbf{Start with the most general rule possible:}
        \begin{itemize}
          \item Condition = empty.
        \end{itemize}
        \item \textbf{Add new attributes by adopting a greedy depth-first strategy.}
        \begin{itemize}
          \item Pick the one that improves the rule quality most.
          \item Current rule $R$: IF condition THEN class = c.
          \item New rule $R'$: IF condition' THEN class = c,
          \item $pos/neg$ are $\#$ of positive/negative tuples covered by $R$.
        \end{itemize}
        \item \textbf{Rule-quality measures.}
        \begin{itemize}
          \item Must consider both coverage and accuracy.
          \item \texttt{FOIL\_Gain} (from \texttt{FOIL} â First-Order Inductive Learner):
          \begin{align}
            \text{FOIL\_Gain} = \text{pos}' \left( \log_2 \frac{\text{pos}'}{\text{pos}' + \text{neg}'} - \log_2 \frac{\text{pos}}{\text{pos}+\text{neg}} \right).
          \end{align}
          \item Favors rules that have high accuracy and cover many positive tuples.
        \end{itemize}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Rule pruning}
      \begin{itemize}
        \item \textbf{Danger of {\color{airforceblue}overfitting}.}
        \item \textbf{Removing a conjunct (attribute test),}
        \begin{itemize}
          \item if pruned version of rule has greater quality,\\
                assessed on an independent set of test tuples (called "pruning set").
        \end{itemize}
        \item \textbf{FOIL uses:}
              \begin{align}
                \text{FOIL\_Prune}(R) = \frac{\text{pos}-\text{neg}}{\text{pos}+\text{neg}}.
              \end{align}
        \item If $\text{FOIL\_Prune}$ is higher for the pruned version of $R$, prune $R$.
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Chapter VI: Classification}
        \begin{itemize}
            \item Classification: basic concepts.
            \item Decision-tree induction.
            \item Bayes classification methods.
            \item Rule-based classification.
            \item \textbf{Model evaluation and selection.}
            \item Techniques to improve classification accuracy: ensemble methods.
            \item Summary.
        \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Model evaluation and selection}
        \begin{itemize}
          \item \textbf{Evaluation metrics:}
          \begin{itemize}
            \item How can we measure accuracy?
            \item Other metrics to consider?
          \end{itemize}
          \item \textbf{Use {\color{airforceblue}test} set of class-labeled tuples instead of training set when assessing accuracy.}
          \item \textbf{Methods for estimating a classifier's accuracy:}
          \begin{itemize}
            \item Holdout method, random subsampling.
            \item Cross-validation.
            \item Bootstrap.
          \end{itemize}
          \item \textbf{Comparing classifiers:}
          \begin{itemize}
            \item Confidence intervals.
            \item Cost-benefit analysis and ROC curves.
          \end{itemize}
        \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Model evaluation and selection}
        \begin{itemize}
          \item \textbf{Confusion Matrix:}\\[0.2cm]
          \begin{tabular}{|c|c|c|}
            \hline
            Actual class/predicted class: & $C_1$ & $\neg C_1$ \\\hline
            $C_1$ & \textbf{True positives (TP)} & \textbf{True negatives (TN)} \\\hline
            $\neg C_1$ & \textbf{False positives (FP)} & \textbf{False negatives (FN)} \\\hline
          \end{tabular}
          \item \textbf{Example:}\\[0.2cm]
          \begin{tabular}{|c|c|c|c|}
            \hline
            Actual class/predicted class: & buys\_computer = yes & buys\_computer = no & Total \\\hline
            buys\_computer = yes & \textbf{6954} & \textbf{46} & 7000 \\\hline
            buys\_computer = no & \textbf{412} & \textbf{2588} & 3000 \\\hline
            Total & 7366 & 2634 & 10000 \\\hline
          \end{tabular}
          \item Given $M$ classes, an entry $C^{(m)}_{ij}$ in an $M \times M$ confusion matrix indicates \# of tuples in class $i$ that were labeled by the classifier as class $j$.
          \item May have extra rows/columns to provide totals.
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Classifier-evaluation metrics: accuracy, error rate, sensitivity and specificity}
      \begin{columns}
        \begin{column}{0.5\textwidth}
          \centering
          \begin{tabular}{|c|c|c|c|}
            \hline
            A/P & C & $\neg$ C & \\\hline
            C & \textbf{TP} & \textbf{FN} & \textbf{P}\\\hline
            $\neg$ C & \textbf{FP} & \textbf{TN} & \textbf{N} \\\hline
            & \textbf{P}' & \textbf{N}' & \textbf{All}\\\hline
          \end{tabular}
          \begin{itemize}
            \item \textbf{Classifier accuracy, or recognition rate:}
            \begin{itemize}
              \item Percentage of test set tuples that are correctly classified.
              \item $\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{All}}.$
              \item \textbf{Error rate:}
                \begin{itemize}
                  \item $1-\text{accuracy}$, or
                  \item $\text{Error rate} = \frac{\text{FP}+\text{FN}}{\text{All}}.$
                \end{itemize}
            \end{itemize}
          \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
          \begin{itemize}
            \item \textbf{Class-imbalance problem:}
            \begin{itemize}
              \item One class may be rare e.g. fraud, or HIV-positive.
              \item Significant majority of the negative class and minority of the positive class
              \item \textbf{Sensitivity:} True-positive recognition rate. $\text{Sensitivity} = \frac{TP}{P}$.
              \item \textbf{Specificity:} False-negative recognition rate. $\text{Specificity} = \frac{TN}{N}$.
            \end{itemize}
          \end{itemize}
        \end{column}
      \end{columns}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Classifier-evaluation metrics: precision, recall, and F-measures}
      \begin{itemize}
        \item \textbf{Precision:}
        \begin{itemize}
          \item Exactness -- the $\%$ of tuples that are actually positive \\
          in those that the classifier labeled as positive: $\frac{\text{TP}}{\text{TP} + \text{FP}}$.
        \end{itemize}
        \item \textbf{Recall:}
        \begin{itemize}
          \item Completeness -- the $\%$ of tuples that the classifier labeled \\
          as positive in all positive tuples: $\frac{\text{TP}}{\text{TP} + \text{FN}}$.
          \item Perfect score is $1.0$.
        \end{itemize}
        \item \textbf{Inverse relationship between precision and recall.}
        \item \textbf{F-measure ($F_1$ or $F$-score):}
        \begin{itemize}
          \item Gives equal weight to precision and recall: $F = \frac{2\cdot\text{precision}\cdot \text{recall}}{\text{precision} + \text{recall}}$.
        \end{itemize}
        \item \textbf{$F_\beta$ weighted measure of precision and recall:}
        \begin{itemize}
          \item Assigns $\beta$ times as much weight to recall as to precision: $F_\beta = \frac{(1+\beta)^2 \cdot \text{precision} \cdot \text{recall}}{\beta^2 \cdot \text{precision} + \text{recall}}$.
        \end{itemize}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Classifier-evaluation metrics: precision, recall, and F-measures}
      \centering
      \begin{tabular}{|c|c|c|c|c|}
        \hline
        Actual class/predicted class & cancer = yes & cancer = no & Total & Recognition ($\%$) \\\hline
        cancer = yes & \textbf{90} & \textbf{210} & 300 & 30.00 (sensitivity) \\\hline
        cancer = no & \textbf{140} & \textbf{9560} & 9700 & 98.56 (specificity) \\\hline
        Total & 230 & 9770 & 10000 & 96.40 (accuracy) \\\hline
      \end{tabular}\\[0.2cm]
      \begin{itemize}
        \item Precision $= \frac{90}{230} = 39.13 \%$.
        \item Recall $=\frac{90}{300} = 30.00 \%$.
        \item F-measure = $2 \cdot 0.3913 \cdot \frac{0.3}{0.3913 + 0.3} = 33.96 \%$.
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Classifier-evaluation metrics: holdout \& cross-validation methods}
      \begin{itemize}
        \item \textbf{Holdout method.}
        \begin{itemize}
          \item Given data is randomly partitioned into two independent sets:
          \begin{itemize}
            \item \textbf{\color{airforceblue}Training set} (e.g. $2/3$) for model construction.
            \item \textbf{\color{airforceblue}Test set} (e.g. $1/3$) for accuracy estimation.
          \end{itemize}
          \item Random sampling: a variation of holdout.
          \begin{itemize}
            \item Repeat holdout $k$ times, accuracy = avg. of the accuracies obtained.
          \end{itemize}
        \end{itemize}
        \item \textbf{{\color{airforceblue}Cross-validation} ($k$-fold, where $k = 10$ is most popular).}
        \begin{itemize}
          \item Randomly partition the data into $k$ mutually exclusive subsets, each approximately equal size.
          \item At $i$-th iteration, use $D_i$ as test set and the others as training set.
          \item Leave-one-out: $k$ folds, where $k = \#$ of tuples; for small-sized data.
          \item Stratified cross-validation: Folds are stratified so that class distribution in each fold is approx. The same as that in the initial data.
        \end{itemize}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Evaluating classifier accuracy: bootstrap}
      \begin{itemize}
        \item \textbf{Bootstrap.}
        \begin{itemize}
          \item Works well with small data sets.
          \item Samples the given training tuples uniformly with replacement.
          \begin{itemize}
            \item I.e. each time a tuple is selected, it is equally likely \\
            to be selected again and re-added to the training set.
          \end{itemize}
        \end{itemize}
        \item \textbf{Several bootstrap methods, and a common one is $.632$ bootstrap.}
        \begin{itemize}
          \item Data set with $d$ tuples sampled d times, with replacement, \\
          resulting in a training set of $d$ samples.
          \item The data tuples that did not make it into the training set end up forming the test set.
          \begin{itemize}
            \item About $63.2\%$ of the original data end up in the bootstrap, and the remaining $36.8\%$ form the test set (since $(1-\frac{1}{d})^d \approx e^{-1} = 0.368)$.
          \end{itemize}
          \item Repeat the sampling procedure $k$ times; overall accuracy of the model:
          \begin{align}
            \text{Acc}(M) = \frac{1}{k} \sum_{i=1}^{k} 0.632 \cdot \text{Acc}(M_i)_{\text{test\_set}} + 0.368 \cdot \text{Acc}(M_i)_{\text{train\_set}}.
          \end{align}
        \end{itemize}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Evaluating classifier accuracy: bootstrap}
      \begin{itemize}
        \item \textbf{Suppose we have $2$ classifiers, $M_1$ and $M_2$, which one is better?}
        \item \textbf{Use $10$-fold cross-validation to obtain $\overline{\text{err}}(M_1)$ and $\overline{\text{err}}(M_2)$.}
        \begin{itemize}
          \item Recall: error rate is $1-\text{accuracy}(M)$.
        \end{itemize}
        \item \textbf{Mean error rates:}
        \begin{itemize}
          \item Just estimates of error on the true population of future data cases.
        \end{itemize}
        \item \textbf{What if the difference between the $2$ error rates is just attributed to chance?}
        \begin{itemize}
          \item Use a test of statistical significance.
          \item Obtain confidence limits for our error estimates.
        \end{itemize}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Evaluating classifier accuracy: null hypothesis}
      \begin{itemize}
        \item \textbf{Perform $10$-fold cross-validation.}
        \begin{itemize}
          \item $10$ times.
        \end{itemize}
        \item \textbf{Assume samples follow a $t$-distribution with $k-1$ degrees of freedom.}
        \begin{itemize}
          \item Here, $k = 10$.
        \end{itemize}
        \item \textbf{Use $t$-test}
        \begin{itemize}
          \item Student's $t$-test.
        \end{itemize}
        \item \textbf{Null hypothesis:}
        \begin{itemize}
          \item $M_1 \& M_2$ are the same.
        \end{itemize}
        \item \textbf{If we can reject the null hypothesis, then}
        \begin{itemize}
          \item Conclude that difference between $M_1 \& M_2$ is statistically significant.
          \item Obtain confidence limits for our error estimates.
        \end{itemize}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Estimating confidence intervals: $t$-test}
      \begin{itemize}
        \item \textbf{If only one test set available: pairwise comparison:}
        \begin{itemize}
          \item For $i$-th round of $10$-fold cross-validation, the same cross partitioning is used to obtain $\text{err}(M_1)_i$ and $\text{err}(M_2)_i$.
          \item Average over $10$ rounds to get $\overline{\text{err}}(M_1)_i$ and $\overline{\text{err}}(M_2)_i$.
          \item $t$-test computes $t$-statistic with $k-1$ degrees of freedom:
                \begin{align}
                  \resizebox{3cm}{!}{%
                  $t = \frac{\overline{\text{err}}(M_1)- \overline{\text{err}}(M_2)}{\sqrt{\frac{\text{var}(M_1-M_2)}{k}}},$}
                \end{align}
          \item where
                \begin{align}
                  \resizebox{10cm}{!}{%
                  $\text{var}(M_1-M_2) = \frac{1}{k} \sum_{i=1}^{k} \left[ \overline{\text{err}}(M_1)_i - \overline{\text{err}}(M_2)_i - (\overline{\text{err}}(M_1)_i - \overline{\text{err}}(M_2)_i)\right]^2.$}
                \end{align}
        \end{itemize}
        \item \textbf{If two test sets available: use nonpaired $t$-test:}
              \begin{align}
                \resizebox{5cm}{!}{%
                $\text{var}(M_1-M_2) = \sqrt{\frac{\text{var}(M_1)}{k_1} + \frac{\text{var}(M_2)}{k_2}},$}
              \end{align}
              where $k_1$ \& $k_2$ are $\#$ of cross-validation samples used for $M_1$ \& $M_2$, respectively.
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Estimating confidence intervals: table for $t$-distribution}
      \begin{columns}
        \begin{column}{0.5\textwidth}
          \vspace{-6cm}

          \centering
          \includegraphics[width=0.8\textwidth]{img/ttest1.jpeg}
          \begin{itemize}
            \item Symmetrical.
            \item \textbf{\color{airforceblue}Significance level}:
            \begin{itemize}
              \item E.g. $\text{sig} = 0.05$ or $5\%$ means $M_1$ \& $M_2$ are significantly different for $95\%$ of population.
            \end{itemize}
            \item Confidence limit: $z = \frac{\text{sig}}{2}$.
          \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
          \centering
          \includegraphics[width=0.7\textwidth]{img/ttest2.jpeg}
        \end{column}
      \end{columns}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Estimating confidence intervals: statistical significance}
      \begin{itemize}
        \item \textbf{Are $M_1$ \& $M_2$ {\color{airforceblue} significantly different}?}
        \begin{itemize}
          \item Compute $t$. Select significance level (e.g. sig = $5\%$).
          \item Consult table for $t$-distribution:
          \begin{itemize}
            \item Find $t$ value corresponding to $k-1$ degrees of freedom (here, $9$).
          \end{itemize}
          \item $t$-distribution is symmetrical:
          \begin{itemize}
            \item Typically upper $\%$ points of distribution shown \\
                  $\implies$ look up value for confidence limit $z = \frac{\text{sig}}{2}$ (here, $0.025$).
          \end{itemize}
          \item If $t > z$ or $t < -z$, then $t$ value lies in rejection region:
                \begin{itemize}
                  \item \textbf{Reject null hypothesis} that mean error rates of $M_1$ \& $M_2$ are equal.
                  \item Conclude: \textbf{statistically significant difference} between $M_1$ \& $M_2$.
                \end{itemize}
          \item Otherwise, conclude that any difference is chance.
        \end{itemize}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Model selection: ROC curves}
      \begin{columns}
        \begin{column}{0.6\textwidth}
          \begin{itemize}
            \item \textbf{ROC (Receiver Operating Characteristics) curves:}
            \begin{itemize}
              \item For visual comparison of classification models.
              \item Originated from signal-detection theory.
              \item Shows the trade-off between the true-positive rate and the false-positive rate.
            \end{itemize}
            \item \textbf{The area under the ROC curve is a {\color{airforceblue}measure of the accuracy} of the model.}
            \item \textbf{{\color{airforceblue}Rank the test tuples} in decreasing order:}
            \begin{itemize}
              \item The one that is most likely to belong to the positive class appears at the top of the list.
            \end{itemize}
            \item \textbf{The closer to the diagonal line (i.e. the closer the area is to $0.5$), the less accurate is the model.}
          \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}

        \end{column}
      \end{columns}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Issues after model selection}
      \begin{itemize}
        \item \textbf{Accuracy.}
        \begin{itemize}
          \item Classifier accuracy: predicting class label.
        \end{itemize}
        \item \textbf{Speed.}
        \begin{itemize}
          \item Time to construct the model (training time).
          \item Time to use the model (classification/prediction time).
        \end{itemize}
        \item \textbf{Robustness.}
        \begin{itemize}
          \item Handling noise and missing values.
        \end{itemize}
        \item \textbf{Scalability.}
        \begin{itemize}
          \item Efficiency in disk-resident databases.
        \end{itemize}
        \item \textbf{Interpretability.}
        \begin{itemize}
          \item Understanding and insight provided by the model.
        \end{itemize}
        \item \textbf{Other measures.}
        \begin{itemize}
          \item E.g. goodness of rules, such as decision-tree size or compactness of classification rules.
        \end{itemize}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{footline}{}
    \begin{frame}{Chapter VI: Classification}
        \begin{itemize}
            \item Classification: basic concepts.
            \item Decision-tree induction.
            \item Bayes classification methods.
            \item Rule-based classification.
            \item Model evaluation and selection.
            \item \textbf{Techniques to improve classification accuracy: ensemble methods.}
            \item Summary.
        \end{itemize}
    \end{frame}
  }

  { % Questions?
    \setbeamertemplate{footline}{}
    \begin{frame}[c]
      \begin{center}
        Thank you for your attention.\\
        {\bf Any questions about the sixt chapter?}\\[0.5cm]
        Ask them now, or again, drop me a line: \\
        \faSendO \ \texttt{luciano.melodia@fau.de}.
      \end{center}
    \end{frame}
  }
\end{document}
